{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen Scala\n",
    "**Membres du group**\n",
    "\n",
    "* Penda diouf\n",
    "* kaynou Camara\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importation des modules Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mrootLogger\u001b[39m: \u001b[32mLogger\u001b[39m = org.apache.log4j.spi.RootLogger@5ecd5e47"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Importation des modules Spark\n",
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` \n",
    "import $ivy.`sh.almond::almond-spark:0.10.9` \n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val rootLogger = Logger.getRootLogger()\n",
    "rootLogger.setLevel(Level.ERROR)\n",
    "\n",
    "Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)\n",
    "Logger.getLogger(\"org.spark-project\").setLevel(Level.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Création de la Session Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/26 17:48:37 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{SparkSession, DataFrame}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@d69f45c"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Création de la Session Spark\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"myApp\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chargement des données (DataFrames)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mjava.lang.reflect.InaccessibleObjectException: Unable to make field private transient java.lang.String java.net.URI.scheme accessible: module java.base does not \"opens java.net\" to unnamed module @48297366\u001b[39m\r\n  java.lang.reflect.AccessibleObject.throwInaccessibleObjectException(\u001b[32mAccessibleObject.java\u001b[39m:\u001b[32m387\u001b[39m)\r\n  java.lang.reflect.AccessibleObject.checkCanSetAccessible(\u001b[32mAccessibleObject.java\u001b[39m:\u001b[32m363\u001b[39m)\r\n  java.lang.reflect.AccessibleObject.checkCanSetAccessible(\u001b[32mAccessibleObject.java\u001b[39m:\u001b[32m311\u001b[39m)\r\n  java.lang.reflect.Field.checkCanSetAccessible(\u001b[32mField.java\u001b[39m:\u001b[32m181\u001b[39m)\r\n  java.lang.reflect.Field.setAccessible(\u001b[32mField.java\u001b[39m:\u001b[32m175\u001b[39m)\r\n  org.apache.spark.util.SizeEstimator$.$anonfun$getClassInfo$2(\u001b[32mSizeEstimator.scala\u001b[39m:\u001b[32m336\u001b[39m)\r\n  org.apache.spark.util.SizeEstimator$.$anonfun$getClassInfo$2$adapted(\u001b[32mSizeEstimator.scala\u001b[39m:\u001b[32m330\u001b[39m)\r\n  scala.collection.IndexedSeqOptimized.foreach(\u001b[32mIndexedSeqOptimized.scala\u001b[39m:\u001b[32m36\u001b[39m)\r\n  scala.collection.IndexedSeqOptimized.foreach$(\u001b[32mIndexedSeqOptimized.scala\u001b[39m:\u001b[32m33\u001b[39m)\r\n  scala.collection.mutable.ArrayOps$ofRef.foreach(\u001b[32mArrayOps.scala\u001b[39m:\u001b[32m198\u001b[39m)\r\n  org.apache.spark.util.SizeEstimator$.getClassInfo(\u001b[32mSizeEstimator.scala\u001b[39m:\u001b[32m330\u001b[39m)\r\n  org.apache.spark.util.SizeEstimator$.visitSingleObject(\u001b[32mSizeEstimator.scala\u001b[39m:\u001b[32m222\u001b[39m)\r\n  org.apache.spark.util.SizeEstimator$.estimate(\u001b[32mSizeEstimator.scala\u001b[39m:\u001b[32m201\u001b[39m)\r\n  org.apache.spark.util.SizeEstimator$.estimate(\u001b[32mSizeEstimator.scala\u001b[39m:\u001b[32m69\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.SharedInMemoryCache$$anon$1.weigh(\u001b[32mFileStatusCache.scala\u001b[39m:\u001b[32m109\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.SharedInMemoryCache$$anon$1.weigh(\u001b[32mFileStatusCache.scala\u001b[39m:\u001b[32m107\u001b[39m)\r\n  org.spark_project.guava.cache.LocalCache$Segment.setValue(\u001b[32mLocalCache.java\u001b[39m:\u001b[32m2222\u001b[39m)\r\n  org.spark_project.guava.cache.LocalCache$Segment.put(\u001b[32mLocalCache.java\u001b[39m:\u001b[32m2944\u001b[39m)\r\n  org.spark_project.guava.cache.LocalCache.put(\u001b[32mLocalCache.java\u001b[39m:\u001b[32m4212\u001b[39m)\r\n  org.spark_project.guava.cache.LocalCache$LocalManualCache.put(\u001b[32mLocalCache.java\u001b[39m:\u001b[32m4804\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.SharedInMemoryCache$$anon$3.putLeafFiles(\u001b[32mFileStatusCache.scala\u001b[39m:\u001b[32m152\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.InMemoryFileIndex.$anonfun$listLeafFiles$2(\u001b[32mInMemoryFileIndex.scala\u001b[39m:\u001b[32m131\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\r\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(\u001b[32mInMemoryFileIndex.scala\u001b[39m:\u001b[32m129\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(\u001b[32mInMemoryFileIndex.scala\u001b[39m:\u001b[32m91\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(\u001b[32mInMemoryFileIndex.scala\u001b[39m:\u001b[32m67\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m534\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m371\u001b[39m)\r\n  org.apache.spark.sql.DataFrameReader.loadV1Source(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m223\u001b[39m)\r\n  org.apache.spark.sql.DataFrameReader.load(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m211\u001b[39m)\r\n  org.apache.spark.sql.DataFrameReader.load(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m178\u001b[39m)\r\n  ammonite.$sess.cmd12$Helper.<init>(\u001b[32mcmd12.sc\u001b[39m:\u001b[32m3\u001b[39m)\r\n  ammonite.$sess.cmd12$.<init>(\u001b[32mcmd12.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd12$.<clinit>(\u001b[32mcmd12.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "//Chargement des données (en DataFrame)\n",
    "val df_jan = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_January_2019.csv\")\n",
    "\n",
    "val df_feb = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_February_2019.csv\")\n",
    "\n",
    "val df_mar = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_March_2019.csv\")\n",
    "\n",
    "val df_apr = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_April_2019.csv\")\n",
    "\n",
    "val df_may = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_May_2019.csv\")\n",
    "\n",
    "val df_jun = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_June_2019.csv\")\n",
    "\n",
    "val df_jul = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_July_2019.csv\")\n",
    "\n",
    "val df_aug = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_August_2019.csv\")\n",
    "\n",
    "val df_sep = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_September_2019.csv\")\n",
    "\n",
    "val df_oct = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_October_2019.csv\")\n",
    "\n",
    "val df_nov = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_November_2019.csv\")\n",
    "\n",
    "val df_dec = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"Sales_December_2019.csv\")\n",
    "//Jonction de tout les dataframes - (combinedDF)\n",
    "val combinedDF = df_jan.union(df_feb)\n",
    "                        .union(df_mar)\n",
    "                        .union(df_apr)\n",
    "                        .union(df_may)\n",
    "                        .union(df_jun)\n",
    "                        .union(df_jul)\n",
    "                        .union(df_aug)\n",
    "                        .union(df_sep)\n",
    "                        .union(df_oct)\n",
    "                        .union(df_nov)\n",
    "                        .union(df_dec)\n",
    "\n",
    "combinedDF.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encodage des colonnes - Order Date (en timestamp), Price Each (en double), et Quantity (en integer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd13.sc:1: not found: value combinedDF\n",
      "val combinedDF2 = combinedDF.withColumn(\"Order Date\", to_timestamp(col(\"Order Date\"), \"MM/dd/yy HH:mm\"))\n",
      "                  ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "val combinedDF2 = combinedDF.withColumn(\"Order Date\", to_timestamp(col(\"Order Date\"), \"MM/dd/yy HH:mm\"))\n",
    "                    .withColumn(\"Quantity Ordered\", col(\"Quantity Ordered\").cast(\"integer\"))\n",
    "                    .withColumn(\"Price Each\", col(\"Price Each\").cast(\"double\"))\n",
    "\n",
    "\n",
    "//Verification\n",
    "combinedDF2.printSchema()\n",
    "combinedDF2.show(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 - Quelle année a été la meilleure en termes de ventes ? Combien a-t-on gagné cette\n",
    "année-là?**                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd13.sc:1: not found: value combinedDF2\n",
      "val salesByYear = combinedDF2.withColumn(\"Year\", year(col(\"Order Date\")))\n",
      "                  ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// Question 1 : Quelle année a été la meilleure en termes de ventes ? Combien a-t-on gagné cette année-là ?\n",
    "val salesByYear = combinedDF2.withColumn(\"Year\", year(col(\"Order Date\")))\n",
    "  .groupBy(\"Year\")\n",
    "  .agg(sum(\"Price Each\").alias(\"Total Sales\"))\n",
    "  .orderBy(desc(\"Total Sales\"))\n",
    "\n",
    "val bestYear = salesByYear.first().getAs[Int](\"Year\")\n",
    "val bestYearSales = BigDecimal(salesByYear.first().getAs[Double](\"Total Sales\"))\n",
    "\n",
    "println(s\"La meilleure année où il y'a eu plus de ventes est $bestYear avec un total de ventes : $bestYearSales%.2f USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 - Quel mois a été le meilleur en termes de ventes ? Combien a-t-on gagné ce mois-là ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd13.sc:2: not found: value combinedDF2\n",
      "val salesByMonth = combinedDF2.withColumn(\"Month\", month(col(\"Order Date\")))\n",
      "                   ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// Question 2 : Quel mois a été le meilleur en termes de ventes ? Combien a-t-on gagné ce mois-là ?\n",
    "import java.time.Month\n",
    "val salesByMonth = combinedDF2.withColumn(\"Month\", month(col(\"Order Date\")))\n",
    "  .groupBy(\"Month\")\n",
    "  .agg(sum(\"Price Each\").alias(\"Total Sales\"))\n",
    "  .orderBy(desc(\"Total Sales\"))\n",
    "\n",
    "val bestMonth = salesByMonth.first().getAs[Int](\"Month\")\n",
    "val bestMonthSales = BigDecimal(salesByMonth.first().getAs[Double](\"Total Sales\"))\n",
    "//fonction pour afficher le nom du mois (en Français)\n",
    "   def afficherMois( a:Int ) : String = {\n",
    "    a match {\n",
    "      case 1  => \"Janvier\"\n",
    "      case 2  => \"Fevrier\"\n",
    "      case 3  => \"Mars\"\n",
    "      case 4  => \"Avril\"\n",
    "      case 5  => \"Mai\"\n",
    "      case 6  => \"Juin\"\n",
    "      case 7  => \"Juillet\"\n",
    "      case 8  => \"Août\"\n",
    "      case 9  => \"Septembre\"\n",
    "      case 10 => \"Octobre\"\n",
    "      case 11 => \"Novembre\"\n",
    "      case 12 => \"Decembre\"\n",
    "      case _  => \"Invalide\"\n",
    "    }\n",
    "   }\n",
    "val bestMonthName = afficherMois(bestMonth)\n",
    "\n",
    "println(s\"Le meilleur mois où il y'a eu plus de ventes est le mois de $bestMonthName avec un total de ventes : $bestMonthSales%.2f USD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 - Quelle ville a enregistré le plus grand nombre de ventes ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd13.sc:4: not found: value combinedDF2\n",
      "val salesDataWithCity = combinedDF2.withColumn(\"City\", trim(split(col(\"Purchase Address\"), \",\")(1)))\n",
      "                        ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Créez une colonne \"City\" à partir de la colonne \"Purchase Address\"\n",
    "val salesDataWithCity = combinedDF2.withColumn(\"City\", trim(split(col(\"Purchase Address\"), \",\")(1)))\n",
    "\n",
    "// Comptez le nombre de ventes par ville\n",
    "val salesByCity = salesDataWithCity.groupBy(\"City\").count()\n",
    "\n",
    "// Trouvez la ville avec le plus grand nombre de ventes\n",
    "val cityWithMostSalesRow = salesByCity.orderBy(desc(\"count\")).first()\n",
    "val cityWithMostSales = cityWithMostSalesRow.getAs[String](\"City\")\n",
    "val numSales = cityWithMostSalesRow.getAs[Long](\"count\")\n",
    "\n",
    "// Affichez la ville avec le plus grand nombre de ventes et le nombre de ventes en chiffres\n",
    "println(s\"La ville avec le plus grand nombre de ventes est : $cityWithMostSales\")\n",
    "println(s\"Le nombre de ventes pour cette ville est : $numSales\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 - À quelle heure devraient-ils diffuser des publicités pour maximiser les chances que les clients achètent le produit ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd13.sc:1: not found: value combinedDF2\n",
      "val salesByHour = combinedDF2.withColumn(\"Hour\", hour(col(\"Order Date\")))\n",
      "                  ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// Question 4 : À quelle heure devraient-ils diffuser des publicités pour maximiser les chances que les clients achètent le produit ?\n",
    "val salesByHour = combinedDF2.withColumn(\"Hour\", hour(col(\"Order Date\")))\n",
    "  .groupBy(\"Hour\")\n",
    "  .count()\n",
    "  .orderBy(desc(\"count\"))\n",
    "\n",
    "val bestHour = salesByHour.first().getAs[Int](\"Hour\")\n",
    "\n",
    "println(s\"Il est recommandé de diffuser des publicités à $bestHour H pour maximiser les chances d'achat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 : Quels produits sont le plus souvent vendus ensemble ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd13.sc:1: not found: value combinedDF\n",
      "val groupedProducts = combinedDF.groupBy(\"Order ID\").agg(collect_list(\"Product\").as(\"Products\"))\n",
      "                      ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// Question 5 : Quels produits sont le plus souvent vendus ensemble ?\n",
    "\n",
    "// Groupez les produits par numéro de commande et collectez les produits dans un tableau\n",
    "val groupedProducts = combinedDF.groupBy(\"Order ID\").agg(collect_list(\"Product\").as(\"Products\"))\n",
    "\n",
    "// Filtrez les commandes avec plus d'un produit\n",
    "val multiProductOrders = groupedProducts.filter(size(col(\"Products\")) > 1)\n",
    "\n",
    "// Effectuez une jointure sur les commandes pour obtenir les combinaisons de produits\n",
    "val productCombinations = multiProductOrders.join(multiProductOrders.withColumnRenamed(\"Products\", \"Products_1\"), Seq(\"Order ID\"))\n",
    "  .where(size(array_intersect(col(\"Products\"), col(\"Products_1\"))) > 1)\n",
    "  .select(col(\"Products\").as(\"Combination\"))\n",
    "  .distinct()\n",
    "\n",
    "// Affichez les combinaisons de produits les plus souvent vendues ensemble\n",
    "println(\"Les produits les plus souvent vendus ensemble :\")\n",
    "productCombinations.show(false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 : Quel produit s'est le plus vendu ? Pourquoi pensez-vous qu'il se soit autant vendu ? (Réponse A)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd13.sc:1: not found: value combinedDF2\n",
      "val bestSellingProduct = combinedDF2.groupBy(\"Product\")\n",
      "                         ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// Question 6 : Quel produit s'est le plus vendu ? Pourquoi pensez-vous qu'il se soit autant vendu ?\n",
    "val bestSellingProduct = combinedDF2.groupBy(\"Product\")\n",
    "  .agg(sum(\"Quantity Ordered\").alias(\"Total Quantity\"))\n",
    "  .orderBy(desc(\"Total Quantity\"))\n",
    "  .first()\n",
    "  .getAs[String](\"Product\")\n",
    "\n",
    "println(s\"Le produit le plus vendu est: $bestSellingProduct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 (Réponse B)** - La raison pour laquelle les piles AAA se sont autant vendu est parce que la plus part des produit sur le catalogue nécessitent des batteries pour fonctionner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 : Quelle est la probabilité que les prochains clients commandent un câble de chargement USB-C ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd13.sc:1: not found: value combinedDF2\n",
      "val totalOrders = combinedDF2.select(\"Order ID\").distinct().count()\n",
      "                  ^cmd13.sc:2: not found: value combinedDF2\n",
      "val usbCOrders = combinedDF2.filter(col(\"Product\").contains(\"USB-C Charging Cable\")).count()\n",
      "                 ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// Question 7 : Quelle est la probabilité que les prochains clients commandent un câble de chargement USB-C ?\n",
    "val totalOrders = combinedDF2.select(\"Order ID\").distinct().count()\n",
    "val usbCOrders = combinedDF2.filter(col(\"Product\").contains(\"USB-C Charging Cable\")).count()\n",
    "val usbCProbability = usbCOrders.toDouble / totalOrders.toDouble\n",
    "\n",
    "println(s\"La probabilité que les prochains clients commandent un câble de chargement USB-C est de $usbCProbability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8 : Quelle est la probabilité que les prochains clients commandent un iPhone ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd13.sc:1: not found: value combinedDF2\n",
      "val totalOrders = combinedDF2.select(\"Order ID\").distinct().count()\n",
      "                  ^cmd13.sc:2: not found: value combinedDF2\n",
      "val iPhoneOrders = combinedDF2.filter(col(\"Product\").contains(\"iPhone\")).count()\n",
      "                   ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// Question 8 : Quelle est la probabilité que les prochains clients commandent un iPhone ?\n",
    "val totalOrders = combinedDF2.select(\"Order ID\").distinct().count()\n",
    "val iPhoneOrders = combinedDF2.filter(col(\"Product\").contains(\"iPhone\")).count()\n",
    "val iPhoneProbability = iPhoneOrders.toDouble / totalOrders.toDouble\n",
    "\n",
    "println(s\"La probabilité que les prochains clients commandent un iPhone est de $iPhoneProbability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9 : Quelle est la probabilité que les prochains clients commandent un téléphone Google ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd13.sc:1: not found: value combinedDF2\n",
      "val totalOrders = combinedDF2.select(\"Order ID\").distinct().count()\n",
      "                  ^cmd13.sc:2: not found: value combinedDF2\n",
      "val googlePhoneOrders = combinedDF2.filter(col(\"Product\").contains(\"Google Phone\")).count()\n",
      "                        ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// Question 9 : Quelle est la probabilité que les prochains clients commandent un téléphone Google ?\n",
    "val totalOrders = combinedDF2.select(\"Order ID\").distinct().count()\n",
    "val googlePhoneOrders = combinedDF2.filter(col(\"Product\").contains(\"Google Phone\")).count()\n",
    "val googlePhoneProbability = googlePhoneOrders.toDouble / totalOrders.toDouble\n",
    "\n",
    "println(s\"La probabilité que les prochains clients commandent un téléphone Google est de $googlePhoneProbability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10 - Quelle est la probabilité que d'autres personnes commandent des écouteurs filaires ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd13.sc:1: not found: value combinedDF2\n",
      "val totalOrders = combinedDF2.select(\"Order ID\").distinct().count()\n",
      "                  ^cmd13.sc:2: not found: value combinedDF2\n",
      "val wiredHeadphoneOrders = combinedDF2.filter(col(\"Product\").contains(\"Wired Headphones\")).count()\n",
      "                           ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// Question 9 : Quelle est la probabilité que les prochains clients commandent des écouteurs filaires ?\n",
    "val totalOrders = combinedDF2.select(\"Order ID\").distinct().count()\n",
    "val wiredHeadphoneOrders = combinedDF2.filter(col(\"Product\").contains(\"Wired Headphones\")).count()\n",
    "val wiredHeadphoneProbability = wiredHeadphoneOrders.toDouble / totalOrders.toDouble\n",
    "\n",
    "println(s\"La probabilité que les prochains clients commandent des écouteurs filaires est de $wiredHeadphoneProbability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question - Pratique**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas d'utilisation concret de Machine Learning :\n",
    "Un cas d'utilisation concret pour ces données de ventes pourrait consister à créer un modèle prédictif pour estimer les ventes futures d'un produit spécifique. Par exemple, supposons que vous souhaitiez prédire les ventes mensuelles d'un produit donné sur la base des données historiques de ventes. Vous pourriez utiliser un modèle de régression pour cela.\n",
    "\n",
    "Voici comment vous pourriez procéder :\n",
    "\n",
    "* Préparez les données : Sélectionnez les variables pertinentes pour votre modèle, telles que le mois, le produit et les ventes. Vous devrez peut-être effectuer des transformations supplémentaires, telles que l'encodage des variables catégorielles.\n",
    "\n",
    "* Divisez les données : Divisez votre ensemble de données en un ensemble d'entraînement et un ensemble de test. Vous utiliserez l'ensemble d'entraînement pour entraîner votre modèle et l'ensemble de test pour évaluer sa performance.\n",
    "\n",
    "* Choisissez et entraînez le modèle : Sélectionnez un modèle de régression approprié, tel qu'une régression linéaire ou une régression logistique. Entraînez le modèle en ajustant les coefficients aux données d'entraînement.\n",
    "\n",
    "* Évaluez le modèle : Utilisez l'ensemble de test pour évaluer les performances de votre modèle. Calculez des métriques telles que l'erreur quadratique moyenne (RMSE) ou le coefficient de détermination (R²) pour évaluer à quel point votre modèle prédit les ventes réelles.\n",
    "\n",
    "* Faites des prédictions : Une fois que vous êtes satisfait des performances de votre modèle, vous pouvez l'utiliser pour faire des prédictions sur les ventes futures. Par exemple, vous pourriez estimer les ventes mensuelles pour les prochains mois en utilisant les données actuelles.\n",
    "\n",
    "Il convient de noter que la construction d'un modèle prédictif robuste nécessite souvent plus de données et des techniques plus avancées. Cependant, cet exemple fournit une base pour commencer à explorer l'utilisation de l'apprentissage automatique pour résoudre des problèmes basés sur les données de ventes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
